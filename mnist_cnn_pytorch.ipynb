{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvrl3blpE60kIsLxSDOc74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BanafshehHassani/ML-Handwritten-Digit-Recognition/blob/main/mnist_cnn_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook builds a classic computer vision application for identifying handwritten digits. It's trained using a simple Convolutional Neural Network on the MNIST dataset.\n",
        "\n",
        "Data: MNIST\n",
        "\n",
        "Author: [Banafsheh Hassani](https://https://www.linkedin.com/in/banafsheh-hassani-7b063a129/)\n",
        "\n",
        "More Projects: [**Github**](https://github.com/BanafshehHassani/)\n",
        "\n",
        "PyTorch on a single node\n",
        "This notebook utilizes PyTorch on the Spark driver node to train the neural network on the MNIST handwritten digit recognition data.\n",
        "\n",
        "It's adapted from the PyTorch project under the license with slight modifications in the comments. Thanks to the developers of PyTorch for this example.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lr6KQuI4_k82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Use GPU if available\n",
        "\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "MNIST_DIR = '/tmp/data/mnist'\n",
        "use_cuda = USE_GPU and torch.cuda.is_available()\n",
        "\n",
        "Params = namedtuple('Params', ['batch_size', 'test_batch_size', 'epochs', 'lr', 'momentum', 'seed', 'cuda', 'log_interval'])\n",
        "args = Params(batch_size=64, test_batch_size=1000, epochs=10, lr=0.01, momentum=0.5, seed=1, cuda=use_cuda, log_interval=200)\n",
        "\n",
        "# Data processing-MNIST\n",
        "# - Download\n",
        "# - Shuffle rows\n",
        "# - Create batches\n",
        "# - Standardize the features\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "data_transform_fn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 datasets.MNIST(MNIST_DIR, train=True, download=True,\n",
        "                   transform=data_transform_fn),\n",
        "               batch_size=args.batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST(MNIST_DIR, train=False,\n",
        "                       transform=data_transform_fn),\n",
        "        batch_size=args.test_batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "# Build a CNN model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = Net()\n",
        "model.share_memory()\n",
        "\n",
        "# Training the model\n",
        "\n",
        "def train_epoch(epoch, args, model, data_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
        "                100. * batch_idx / len(data_loader), loss.data.item()))\n",
        "\n",
        "\n",
        "def test_epoch(model, data_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            if args.cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').data.item() # sum up batch loss\n",
        "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data).cpu().sum()\n",
        "\n",
        "    test_loss /= len(data_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(data_loader.dataset),\n",
        "        100. * correct / len(data_loader.dataset)))\n",
        "\n",
        "\n",
        "# Run training loop over epochs, evaluate after each.\n",
        "\n",
        "if args.cuda:\n",
        "    model = model.cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    train_epoch(epoch, args, model, train_loader, optimizer)\n",
        "    test_epoch(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYv6Y4wH_kgO",
        "outputId": "40a99e2f-d25c-4a6f-882e-c6b24954f1f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /tmp/data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 76094425.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /tmp/data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /tmp/data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 19375510.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /tmp/data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /tmp/data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 23681639.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /tmp/data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /tmp/data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6779547.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /tmp/data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /tmp/data/mnist/MNIST/raw\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.371851\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.208917\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.603620\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.730665\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.473970\n",
            "\n",
            "Test set: Average loss: 0.2090, Accuracy: 9401/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.372890\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.382565\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.394691\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.311141\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.346244\n",
            "\n",
            "Test set: Average loss: 0.1251, Accuracy: 9619/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.443347\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.302536\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.300118\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.448292\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.358607\n",
            "\n",
            "Test set: Average loss: 0.1005, Accuracy: 9687/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.200479\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.239514\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.311127\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.319920\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.266833\n",
            "\n",
            "Test set: Average loss: 0.0850, Accuracy: 9722/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.129706\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.278238\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.197259\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.108888\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.213162\n",
            "\n",
            "Test set: Average loss: 0.0732, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.168471\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.202100\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.174319\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.174163\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.152776\n",
            "\n",
            "Test set: Average loss: 0.0685, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.322959\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.227914\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.152297\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.307722\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.125666\n",
            "\n",
            "Test set: Average loss: 0.0630, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.159625\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.304664\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.184048\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.036351\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.141846\n",
            "\n",
            "Test set: Average loss: 0.0595, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.133558\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.192466\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.091611\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.398304\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.180705\n",
            "\n",
            "Test set: Average loss: 0.0531, Accuracy: 9831/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.182651\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.157201\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.430423\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.139940\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.236185\n",
            "\n",
            "Test set: Average loss: 0.0523, Accuracy: 9849/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key information from training output:\n",
        "\n",
        "In the prescribed logging intervals, the training process prints out how many samples it has gone through in the current epoch, as well as the loss of the current training mini-batch. It's"
      ],
      "metadata": {
        "id": "1oWdR94Y_wRq"
      }
    }
  ]
}