{"cells":[{"cell_type":"markdown","source":["# Handwritten Digit Recognition\nThis notebook building a classic computer vision application of identifying handwritten digits. \nIt's train a simple Convolutional Neural Network on the MNIST dataset.\n## Data : MNIST \n\n###[Banafsheh Hassani](https://www.linkedin.com/in/banafsheh-hassani-7b063a129/)\n\n###[More Projects](https://github.com/BanafshehHassani)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0070daa5-c7ed-4b11-9a7a-90cceaf29840"}}},{"cell_type":"markdown","source":["## PyTorch on a single node\n\nThis notebook using PyTorch on the Spark driver node for fit the neural network on the MNIST handwritten digit recognition data.\n\n[copied from the PyTorch project](https://github.com/pytorch/examples/blob/53f25e0d0e2710878449900e1e61d31d34b63a9d/mnist/main.py) under the [license](https://github.com/pytorch/pytorch/blob/a90c259edad1ea4fa1b8773e3cb37240df680d62/LICENSE) with slight modifications in comments. Thanks to the developers of PyTorch for this example."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f40fc76-60e6-4547-a56f-f4b7030f7b09"}}},{"cell_type":"markdown","source":["* [Source 1](https://docs.databricks.com/_static/notebooks/deep-learning/pytorch-single-node.html) \n* [Source 2](https://github.com/pytorch/examples/blob/53f25e0d0e2710878449900e1e61d31d34b63a9d/mnist/main.py)\n* [license](https://github.com/pytorch/pytorch/blob/a90c259edad1ea4fa1b8773e3cb37240df680d62/LICENSE)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bc49634-56bf-4260-92e9-fe19d2853af1"}}},{"cell_type":"code","source":["dbutils.widgets.removeAll()\ndbutils.widgets.dropdown('USE_GPU', 'no', ['no', 'yes'])\nUSE_GPU = dbutils.widgets.get('USE_GPU') == 'yes'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6645926-fedc-4b06-ab9d-0e644a7d55d3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Import libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6baaa694-67aa-4ddd-92f9-c4c72f7a2906"}}},{"cell_type":"code","source":["from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9dbf895-51f8-4da9-9fd3-06f76f64188b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["MNIST_DIR = '/tmp/data/mnist'\nuse_cuda = USE_GPU and torch.cuda.is_available()\n\nParams = namedtuple('Params', ['batch_size', 'test_batch_size', 'epochs', 'lr', 'momentum', 'seed', 'cuda', 'log_interval'])\nargs = Params(batch_size=64, test_batch_size=1000, epochs=10, lr=0.01, momentum=0.5, seed=1, cuda=use_cuda, log_interval=200)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ce4969b-0109-45c1-bfcd-ad6e04b66561"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Data processing-MNIST  \n* Download \n* Shuffle rows \n* Create batches \n* Standardize the features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e521b117-bc2c-4ae9-83ce-747518757e53"}}},{"cell_type":"code","source":["torch.manual_seed(args.seed)\n\ndata_transform_fn = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))])\n\ntrain_loader = torch.utils.data.DataLoader(\n                 datasets.MNIST(MNIST_DIR, train=True, download=True,\n                   transform=data_transform_fn),\n               batch_size=args.batch_size, shuffle=True, num_workers=1)\n\ntest_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(MNIST_DIR, train=False, \n                       transform=data_transform_fn),\n        batch_size=args.test_batch_size, shuffle=True, num_workers=1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b5f0f98-0c08-4398-a0ba-6e70f70ceb04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Build a CNN model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be9f5cf8-7a09-47a8-8fb0-dd46fb8129fd"}}},{"cell_type":"code","source":["class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n      \nmodel = Net()\nmodel.share_memory() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d044e55-e941-4eae-8631-d05e1a02932e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: Net(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=320, out_features=50, bias=True)\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\n)</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: Net(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=320, out_features=50, bias=True)\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\n)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5379c1d-90c9-442e-b7dc-6e636ec416a2"}}},{"cell_type":"code","source":["def train_epoch(epoch, args, model, data_loader, optimizer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(data_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()      \n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(data_loader.dataset),\n                100. * batch_idx / len(data_loader), loss.data.item()))\n\n\ndef test_epoch(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in data_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()      \n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data).cpu().sum()\n\n    test_loss /= len(data_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4a99fd5-d04b-413f-a193-05b186ad542f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Runing training loop over epochs, it evaluate after each."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15bff89a-d635-43ad-a08d-19f9fa98d5a4"}}},{"cell_type":"code","source":["if args.cuda:\n    model = model.cuda()\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\nfor epoch in range(1, args.epochs + 1):\n    train_epoch(epoch, args, model, train_loader, optimizer)\n    test_epoch(model, test_loader)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16c6107a-61fe-4397-9db9-6df7f73c7c35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;command-1194993396842650&gt;:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  return F.log_softmax(x)\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.332552\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 1.125173\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.772878\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.518417\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.485763\n&lt;command-1194993396842652&gt;:25: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  data, target = Variable(data, volatile=True), Variable(target)\n/databricks/python/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction=&#39;sum&#39; instead.\n  warnings.warn(warning.format(ret))\n\nTest set: Average loss: 0.1921, Accuracy: 9420/10000 (94%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.364460\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.502845\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.238232\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.279074\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.446922\n\nTest set: Average loss: 0.1240, Accuracy: 9614/10000 (96%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.231387\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.559281\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.292914\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.401911\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.129268\n\nTest set: Average loss: 0.0984, Accuracy: 9698/10000 (97%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.365490\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.236167\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.215550\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.252271\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.479188\n\nTest set: Average loss: 0.0828, Accuracy: 9742/10000 (97%)\n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.199672\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.149998\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.271687\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.189840\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.446669\n\nTest set: Average loss: 0.0711, Accuracy: 9780/10000 (98%)\n\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.149341\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.335459\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.114738\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.239979\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.201276\n\nTest set: Average loss: 0.0704, Accuracy: 9786/10000 (98%)\n\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.175631\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.083417\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.198340\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.332557\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.203750\n\nTest set: Average loss: 0.0610, Accuracy: 9808/10000 (98%)\n\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.072501\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.057836\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.168418\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.127133\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.403241\n\nTest set: Average loss: 0.0553, Accuracy: 9835/10000 (98%)\n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.123991\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.179392\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.112713\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.129101\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.105125\n\nTest set: Average loss: 0.0536, Accuracy: 9830/10000 (98%)\n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.097767\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.142445\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.218235\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.102758\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.297475\n\nTest set: Average loss: 0.0516, Accuracy: 9846/10000 (98%)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;command-1194993396842650&gt;:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  return F.log_softmax(x)\nTrain Epoch: 1 [0/60000 (0%)]\tLoss: 2.332552\nTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 1.125173\nTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.772878\nTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.518417\nTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.485763\n&lt;command-1194993396842652&gt;:25: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  data, target = Variable(data, volatile=True), Variable(target)\n/databricks/python/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction=&#39;sum&#39; instead.\n  warnings.warn(warning.format(ret))\n\nTest set: Average loss: 0.1921, Accuracy: 9420/10000 (94%)\n\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.364460\nTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.502845\nTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.238232\nTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.279074\nTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.446922\n\nTest set: Average loss: 0.1240, Accuracy: 9614/10000 (96%)\n\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.231387\nTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.559281\nTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.292914\nTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.401911\nTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.129268\n\nTest set: Average loss: 0.0984, Accuracy: 9698/10000 (97%)\n\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.365490\nTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.236167\nTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.215550\nTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.252271\nTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.479188\n\nTest set: Average loss: 0.0828, Accuracy: 9742/10000 (97%)\n\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.199672\nTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.149998\nTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.271687\nTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.189840\nTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.446669\n\nTest set: Average loss: 0.0711, Accuracy: 9780/10000 (98%)\n\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.149341\nTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.335459\nTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.114738\nTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.239979\nTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.201276\n\nTest set: Average loss: 0.0704, Accuracy: 9786/10000 (98%)\n\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.175631\nTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.083417\nTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.198340\nTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.332557\nTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.203750\n\nTest set: Average loss: 0.0610, Accuracy: 9808/10000 (98%)\n\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.072501\nTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.057836\nTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.168418\nTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.127133\nTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.403241\n\nTest set: Average loss: 0.0553, Accuracy: 9835/10000 (98%)\n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.123991\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.179392\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.112713\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.129101\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.105125\n\nTest set: Average loss: 0.0536, Accuracy: 9830/10000 (98%)\n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.097767\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.142445\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.218235\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.102758\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.297475\n\nTest set: Average loss: 0.0516, Accuracy: 9846/10000 (98%)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The key information of training in output:\n- In prescribed logging intervals, training process prints out how many samples in the training set, it has gone through in the current epoch, as well as loss of the current training mini-batch. \n- that's obvious over epochs, on testing set, loss decreases plus accuracy increases, suggesting that model is in overal improving, even though at batch level there might be fluctuations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ea4574b-57ab-4ea0-aa18-ae9562d3e8ee"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"BH_Handwritten Digit Recognition","dashboards":[],"language":"python","widgets":{"USE_GPU":{"nuid":"d2d6a29f-e2ca-4cb1-93dd-f4f95aa16d96","currentValue":"yes","widgetInfo":{"widgetType":"dropdown","name":"USE_GPU","defaultValue":"no","label":null,"options":{"widgetType":"dropdown","choices":["no","yes"]}}}},"notebookOrigID":1194993396842638}},"nbformat":4,"nbformat_minor":0}
